# numc

Here's what I did in project 4:
-
I first wrote up all the arithmetic functions in Matrix.c, which includes set, get, fill, add, subtract, multiply, negative, and absolute value. These functions are the backbone of the numc, all of the matrix manipulations ever needed. 
Then, a setup file is written to set up and install the numc module so that it can be used in python. After that, I wrote the python C API interface in numc.c. This required an thorough understanding of the object types PyObject and PytypeObject. These types and attribute functions allow me to transform C objects to python objects an vice versa. For example, PyLongAsLong turns C floating points into PyObject Long, and PyLongFromLong turns PyObjects into C floating points, such as doubles. 

The interface numc.c is written around the struct Matrix61c, which contains a matrix struct from matrix.c and its shape as a tuple, and turns that Object into a python library module. Along with it, numc.c translates python arithmetics into functions in the interface, for example, a+b, which then call the arithmetic functions in Matrix.c, accomplishing the goal of computing these operations. 

After the interface being written, mumc is ready to be used by importing numc in python scripts. This naive implementation is compared with dumpy, a identical and naive implementation, in a special environment of python 3.6. Their speeds were relatively similar at first.

Lastly, omptimization needs to applied. First, for the simple operations, addition, subtraction, negative, absolute value, and fill matrix, I used intel simd intrinsics, and open MP parallel multithreading techniques. simd intrinsics, for example, can load 4 doubles in parallel each vector, and then add them, and then store them on the the destination matrix, all in parallel. This cuts down the time by 4. Then loop unrolling is applied. I unrolled 32 additions in one loop, and add a tail case to finish the unrolling. The immediately speeds up the operation even more. Lastly, I added the omp parallel for in front of the for loop. I can do this easily because addition is element-wise, and thus each addition is completely independent of each other. Similarly, I can do this for fill matrix, subtraction, negative, and absolute value. These functions act very similarly, as they are all element-wise and each entry is completely independent of each other. This gives me a 5.1x speed up for these operations. 

After that optimization is applied for power and multiply. First, I transposed the second matrix using cache blocking. This allows me not to stride through the matrix when the multiplication is operated. Then, in the nested for loops, I can use again use simd intrinsics the multiply and add the matrices and them, since I donâ€™t need to stride through matrix 2, thanks to the transposition. Another small optimization is that, in the second inner loop, additions are first added to a intermediate vector through all the inner loop, and then at the end of the second inner loop I finally store the intermediate intrinsics vector. This saves the amount of stores done, which speeds up the multiply by about 95 times. 

Then, with multiply optimized, I use a repeated squaring algorithm to optimize power. I used 3 intermediate matrices to store matrices and use the iterative implementation to speed up the process. This at the end gives me a roughly 1800x speed up. 
